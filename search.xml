<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[How to respond to internal audit]]></title>
    <url>%2F2019%2F01%2F18%2FIT-IA%2F</url>
    <content type="text"><![CDATA[How to respond to internal audit? After many years being participating in different audits and acting as an internal auditor, I believe writing this post would help clarify a lot questions people have about audit. More importantly, this is for IT guys to know how to respond an internal audit. know about IT internal auditor How to respond to internal auditors 1. Be honest and transparent 2. Share known gaps and action plans 3. Assume auditors know nothing about your area or expertise 4. Define action plan together with auditors 5. Disagree with supporting evidences and risk explaination Reference IT Internal Auditor Persona - Ivy Values - What value Ivy brings to the company? Goals - What Ivy wants to achieve? Needs - What Ivy needs to do? Frustrations - What makes Ivy frustrated? Pain Points - What pain points does Ivy have? Motivators - What motivates Ivy to take certain actions? know about IT internal auditor Before stepping into any practical tips, let’s know something about IT internal auditor firstly. I built a persona for this group of peoples, and I call her Ivy. I put words version to reference part to help reading. How to respond to internal auditors Assuming you have read through above persona, as it explains why I put following tips. 1. Be honest and transparent Internal auditors are exactly as same as you, as employees of the company aiming to add value and know how this company works. Sometimes, they move to internal auditor position from your function area, maybe used to be former you. If you try to hide something or are bluffing, you just let them doubt your credibility and let them question everything you say after (even you may answer honestly to other questions). When auditors find the real answer, you will make yourself silly. For sure, you can keep denying, but man, what is the value ? Take care of your reputation within the company. 2. Share known gaps and action plans If you have already known some gaps and made action plans or projects to remediate, don’t wait for auditors to find them out, share from the beginning. For known issues with proper action plans in place, they normally should not seen as audit issues. But, please do not try playing clever on this point. For example, planning to fix a critical issue with a super long time period, fix partial issue, or making a fake plan, for sure you deserve audit issues. 3. Assume auditors know nothing about your area or expertise Auditors cannot be experts in everything, that means they may not understand how the tool or protocol works, or why a process is designed as such. They would ask you step by step how it works, and may go back to previous questions many times. Conversation seems very simple and even boring. When you have been working in this area for many years, you may not notice some steps you take and don’t see the risk. Auditors have good risk mindset, they try to break processes down to see where it may go wrong. To those potential risky points, they will try materializing them by analyzing history data and logs, and doing tests, etc. So, help auditors know all the details and they could help you to identify risks, it’s win-win. 4. Define action plan together with auditors If you think you must do what auditors ask for, I have to say this is a wrong perception. Auditors are good at risk, but they may not know as much as you do in your area or expertise. Auditors normally give you suggestions and seek for you feedback. Don’t doubt about it, they really need your feedback, not just to be polite. Take some time to work together with them to define an action plan making sense to both sides. Once an audit report is finalized and distributed to management team, it will be very difficult to change the action plan or the target date. 5. Disagree with supporting evidences and risk explaination As said previously, auditors are sharp and know how to identify risks, it does not mean their opinions are always right. When you strongly believe that the audit issue is not true, debate with supporting evidence and ideally prove the risk scenario described by auditors would not stand. Just saying ‘No, I don’t think so’ or speaking loud will not make your opinion strong or make anyone agree with you. It will only makes you like a rude and cueless child. In simple word, try to be constructive. That’s all for now, I’ll come back to update when new tips come up. Good Luck ! Reference IT Internal Auditor Persona - Ivy Values - What value Ivy brings to the company? An independent challenger, helping the company to build a strong system to control IT risks Goals - What Ivy wants to achieve? Provide independent assurance to challenge the ‘status quo’. Drive IT organization improving the risk management maturity. Raise awareness of technology and improve risk mindset across the company. Needs - What Ivy needs to do? Define review plan via analyzing key risks, technology trend, IT strategy and road-map. Appraise risks and good practices via performing review, assessment and/or advisory missions in technology area. Develop pragmatic recommendations on risk remediation action plans, and follow up on the implementation. Report conclusions to management team and recommend corrective actions and other improvement suggestions to improve the effectiveness and efficiency of risk controls. Contribute to the development of IT governance activities throughout the company, including ongoing development of IT controls frameworks for key IT related risk areas, risk management frameworks, product/project management methodologies and audit methodologies. Foster and maintain effective and constructive relationships with key stakeholders. Improve the effectiveness and efficiency of way of working within own function, such as the way of performing review, reporting, data analytics, testing, risk evaluation. Instill to management team and wide range of stakeholders about IT controls, policies, standards, good practices and key risks. Frustrations - What makes Ivy frustrated? Low level of cooperation and transparency from stakeholder from a preconception of seeing auditor as a trouble maker or seeing audit missions as non-relevant work. Audit issues are closed while the implementation is not fully in place or in a sustainable way. Lose track of action implementation due to owner changes within IT organization. Low priority actions are long-time overdue. Pain Points - What pain points does Ivy have? Lack of good quantitative risk rating method and the pressure of ‘targeting for 0 major audit issue’ lead to long-time dispute. Thereby, risks and actions are not thoroughly discussed. Risks that cannot directly impact security, compliance or core business, such as user experience and productivity, are difficult to be materialized or accepted as audit finding. Some IT auditors may not have technical skills to materialize issues. Motivators - What motivates Ivy to take certain actions? Internal auditors may have target about closing audit issues in certain time period, that they would be driven to raise less critical issues or to close issues partially to meet the target. An audit mission may have to meet certain deadline or have limited resource, that auditors would not test or review all control points.]]></content>
      <tags>
        <tag>internal audit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络安全法和电子商务法里的IT控制要求]]></title>
    <url>%2F2019%2F01%2F16%2FChina-regulations%2F</url>
    <content type="text"><![CDATA[网络安全法和电子商务法里的IT控制要求 说在前面 本文章纯属作者从IT（信息技术）的视角对两个法律的解读，总结出来的内容用于个人学习和讨论的用途，任何单位和个人不得用于商业用途。文章内容的版权和解释权归作者所有。此外，作者不是法律专业人士，不保证解读内容的准确性和完整性，内容不得用于任何与法律有关的学习或者活动。如有任何单位和个人对内容有异议，应该及时向本站书面反馈，并提供身份证明、权属证明或详细情况证明，本站在收到上述文件后将会尽快更新或者移除相关内容。 更多免责条款请参见本站的免责声明。 重要的说明 本文不再赘述有关网络安全法和电子商务法的背景、意义、生效日期、条款内容等基本信息，有兴趣的人请自行Google或者百度。 为了便于比较和分析，我将与IT有关的条款分别打了标签，用于注明其所属的控制领域。如果有人感兴趣具体条款和标签如何对应的，请留言，我会再写一篇文章公布细节。 在后续的分析中我会提到哪些控制要求比较有中国特色，哪些属于常见的行业标准。 涉及网络安全的，由于行业标准和各国法规十分多，分类依靠本人长期的工作经验总结所得，就不罗列所有的标准了。如果哪位同学做出了条款和各大行业标准对照表的，请不吝分享。 涉及电子商务的，我参考了英国的The Electronic Commerce (EC Directive) Regulations 2002以及Guide to Privacy and Electronic Communications Regulations和欧盟对电子商务网站的指引。至于为什么只参考了这些呢，Google出来的前几项就是答案 😄 未来有空时，我会去研究研究其他国家或地区的要求的。 涉及电子支付的，我参考了欧盟的Payment services (PSD 2) - Directive (EU) 2015/2366。 因为存在多个条款同属一个控制领域，某些条款比较有中国特色，某些比较行业内常见，所以当你看到我说某一个控制领域是有中国特色的，这不意味着整个控制领域都是中国独有的，请不要急于反驳，建议温和地留言并提出你的意见或者问题。谢谢支持。 由于本人长期使用的工作语言是英语，有时候对专业术语的中文表述不是很准确，请见谅。😉 OK，把重要的免责声明和内容说明都讲清楚了，接下来我就直接开始正题了 😃 1. 控制领域概况 网络安全法 电子商务法 2. 网络安全法 vs 电子商务法 3. 较有中国特色 vs 行业内常见 4. 控制要求的适用范围 1. 控制领域概况 网络安全法 首先网络安全法，我归类出7个大的领域 （排名不分先后）： 数字化安全（Digital Security） 合规 隐私保护 治理体系 分级分类 业务持续性 合同与协议 进一步归类并且统计每一类的要求数量，参见下图。可见，要求较为集中在数字化安全（Digital Security）和合规领域。其中，又以安全事故响应和国家安全审查的要求最多。区别于其他安全规范，网络安全法里特别提及了隐私保护。 作为一个法律文件，它不会提及具体的安全技术或者流程要求，这需要基于安全管理者的经验和认识进行实施。举个例子，第二十一条这么定义的： 第二十一条 国家实行网络安全等级保护制度。网络运营者应当按照网络安全等级保护制度的要求，履行下列安全保护义务，保障网络免受干扰、破坏或者未经授权的访问，防止网络数据泄露或者被窃取、篡改： （一）制定内部安全管理制度和操作规程，确定网络安全负责人，落实网络安全保护责任； （二）采取防范计算机病毒和网络攻击、网络侵入等危害网络安全行为的技术措施； （三）采取监测、记录网络运行状态、网络安全事件的技术措施，并按照规定留存相关的网络日志不少于六个月； （四）采取数据分类、重要数据备份和加密等措施； （五）法律、行政法规规定的其他义务。 所以，到底要求仅仅是列举出来的这5条，还是更多？ 偷懒的做法是只关注这5条并有很强的说辞来坚持这个看法；这种情况其实很常见，尤其当没有足够的预算时。 不过实事求是地看这条要求，重点其实是保障网络免受干扰、破坏或者未经授权的访问，防止网络数据泄露或者被窃取、篡改。虽然访问控制没有在5条里特别说明，我们不能否认这项控制对于达到如上目标的重要性。 电子商务法 然后看电子商务法，我归类出7个大的领域*（同样，排名不分先后）*： 用户权益 合规 电子支付 合同与协议 隐私保护 治理体系 数字化安全（Digital Security） 具体参见下图，可见，用户权益和电子支付被着重提及的领域。数字化安全（Digital Security）看似没有特别描述，但是不要忘记网络安全法哦，合规可不是合一个法律的规就行了。😃 电子商务法的隐私保护和网络安全法的隐私保护有什么区别？ 隐私保护子领域 电子商务法 网络安全法 信息收集 无明显区别 无明显区别 信息透明 服务提供者公示隐私请求管理流程的责任 无特别提及 合法合规 提供信息给有关主管部门的要求 无特别提及 数据分析和推送 使用数据分析推送广告的要求 无特别提及 数据安全 无明显区别 无明显区别 数据泄露 无特别提及 针对数据流露处理和响应的要求 知情同意 无特别提及 收集和使用信息前获得用户同意的要求 隐私请求管理 增加了用户注销的要求 其他处理用户有关隐私的请求的要求无明显区别 我把两个法律要求集中分析了以下，发现关键词出现最频繁的是… 2. 网络安全法 vs 电子商务法 简单地对比一下两个法律的侧重领域，见下图。显然用户权益和电子支付是电子商务法特有的，然而，*（我有点意外）*分级分类和业务持续性竟然在电子商务法没有特别提及。可能电子商务法还是更侧重在商务领域吧。 我的个人看法：即使不具体列举业务持续性的要求，像“数字化安全（Digital Security）”一样提及一下也是必要的。当电子商务对行业发展变得这么重要后，平台的高可用和容灾能力直接影响业务的可用性，这并不比数据安全的重要性低。 3. 较有中国特色 vs 行业内常见 这个章节也是为了抛砖引玉，我可能有理解不到位的地方，仅供大家参考。 没有意外的是合规性要求比较多，例如实名制、资格审查、内容管理、境内储存关键数据、报送有关主管部门等等。 需要特别指出的是，“较有中国特色”是个中性词，这不意味着要求是负面的或总是有关政府的，有些要求还蛮有创意，个人认为挺不错的。举个例子，电子商务法的第三十九条： 第三十九条 电子商务平台经营者应当建立健全信用评价制度，公示信用评价规则，为消费者提供对平台内销售的商品或者提供的服务进行评价的途径。 电子商务平台经营者不得删除消费者对其平台内销售的商品或者提供的服务的评价。 这条关于信用评价制度的要求自然是有益于服务提供者和消费者的。这在很多国内外的电子商务平台可能都有了，但是在法律里规范地写出来，无疑是进步的。同样的例子还有推送广告时要向该消费者提供不针对其个人特征的选项，尊重和平等保护消费者合法权益的要求。 还有几条十分细致的，对实际操作有影响的要求，这样细致的要求我还没在英国或者欧盟的法律文件里看到。（如果我理解错了，这只是由于我的阅读量不够，请指正） 例如： 第十九条 电子商务经营者搭售商品或者服务，应当以显著方式提请消费者注意，不得将搭售商品或者服务作为默认同意的选项。 第二十一条 电子商务经营者按照约定向消费者收取押金的，应当明示押金退还的方式、程序，不得对押金退还设置不合理条件。消费者申请退还押金，符合押金退还条件的，电子商务经营者应当及时退还。 具体看看到底涉及哪些较有中国特色的控制要求… 再附上行业内常见的控制要求作为参考… 4. 控制要求的适用范围 最后，简单地看一下适用范围： 是不是发现要求少了很多，压力稍微减轻了点？😄 再看电子商务法的，按角色对号入座后，要求其实也不是很多。 欢迎讨论]]></content>
      <tags>
        <tag>中国法规</tag>
        <tag>网络安全法</tag>
        <tag>电子商务法</tag>
        <tag>安全合规</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DevOps benchmarking metrics and review framework]]></title>
    <url>%2F2019%2F01%2F11%2FDevSecOps-Measurement%2F</url>
    <content type="text"><![CDATA[Context While DevOps model has been widely adopted by IT organization, I do not find a good framework to centrally measure the maturity of the model as well as the user experience as outcome. So, I try to build one to provide a clear guidance, and I’d like to call it DevOps benchmarking metrics and review framework. This is not created from scratch, but is based on a lot of works and analysis done by different orgs. The major source of reference is DevSecOps guides from GSA. I hope this could be a good reading materials for peoples managing a DevOps team or working in security, compliance and risk areas. This is a high level review guideline, not a cookbook of DevOps or review. If you have any comments or questions, please leave a message below or contact me via email which can be found from my blog. Context What do we want to achieve through DevOps? What to look at for the capabilities we want to achieve? Which metrics can be used to measure these capabilities? Reference Details of review topics Governance pillar Process pillar Demand shaping &amp; biz value tracking User centric design DevSecOps model Agile planning (incl. Change management) CI &amp; CD Testing strategy &amp; quality assurance Logging &amp; monitoring Application security Risk management Business continuity Technology Pillar Automation capability (both biz and IT processes) Resilience of technologies Details of benchmarking metrics Productivity Risk management capability Usability Reliability What do we want to achieve through DevOps? First of all, we expect the team having a sustainable way to improve user experience continuously. User means not only end user, but also other business stakeholders and IT support partners. To achieve that, the key capabilities should be defined from both DevOps model and the product perspectives. I draw a diagram to better explain the relationship between the aim and the key capabilities, as below. What to look at for the capabilities we want to achieve? Ok, now list down the main topics for assessing the capabilities identified above. I group them in 3 pillars: Governance, Process and Technology. The code of each capability (A, B, C, D) is put next to each topic to link them up. Governance Process Technology Team structure &amp; location A C Demand shaping &amp; biz value tracking A C Automation capability(both biz and IT processes) A B D Role &amp; responsibility &amp; people profile (both internal &amp; external resources) A B C D User centric design* User interviews* Industry reviews* Product definition &amp; roadmap* A/B testing A C Resilience of technologies* Risk level (from legal, security, compliance standpoints)* Availability of the 3rd party services* Development/update lifecycle A B D Budget allocation &amp; procurement A B DevSecOps model* Agile planning (incl. Change management)* CI &amp; CD* Testing strategy &amp; quality assurance* Logging &amp; monitoring* Application security* Risk management A B C D Targeting setting A B Business continuity* High availability* Disaster recovery B D Which metrics can be used to measure these capabilities? After all, it’s always preferred by management team to have more visualized way to know the performance of the model or the product. Of course, it’s also important to have some quantitative KPIs to know which areas still needs improvement. However, please be noted that, when we decide to improve certain capability, we should look back to relevant topics and good practices of those topics, but not focus on how to change the value of KPIs. Why? the later way might lead to KPI manipulation and fake assessment. 😉 Below graph gives an overview of metrics under each capability, the detailed definitions can be found in “Reference” section. It’s suggested to focus on the velocity of maturity improvement, than current maturity level. The reason is simple, as we expect the team to keep improving at a comfortable speed, but not be satisfied with what it is now. Reference Details of review topics This could be a good reference to perform a holistic review, I will not further elaborate what to review under each topic. Governance pillar Team structure &amp; location Role &amp; responsibility &amp; people profile (both internal &amp; external resources) Budget allocation &amp; procurement Targeting setting Process pillar Demand shaping &amp; biz value tracking Business cases or KPIs definition, quantitative KPI calculation method Business case tracking and validation Collaboration with business and user story definition User centric design User interviews Industry reviews Product definition &amp; roadmap A/B testing DevSecOps model Agile planning (incl. Change management) The flow of work: what are the steps it takes to get a Story to Done? what is the definition of done? Tracking of the flow Built-in quality: link technical standards to work items Emergent design vs Intentional architecture: project specific design vs long-term strategy Routines for planning: regular personal review and team review, backlog refinement, iteration review, iteration retrospective CI &amp; CD Branching and merging policies Tepository management Dev/Test/Prod environment and pipeline setup Ways of deployments Build and release automation: including all mandatory test/scan/approval/technical requirements steps Testing strategy &amp; quality assurance Testing strategy and mapping with CI/CD Success criteria and target setting Automation tools for all types of tests Code scan: for quality, security including dependencies Manual test and feedback Bug tracking Logging &amp; monitoring Logging requirements and enables Consolidation of logs, alerts, reports Monitoring tools Tracking of security events, errors, performance issues Application security Security automation Security hardening and monitoring at platform layer Secrets (key/token/privileged credentials) protection API and web services security configuration and monitoring Vulnerability identification and patching process Data loss prevention Risk management Pilot from risk perspective Knowledge base and training Dependencies on people and procurement processes Architecture and security review Business continuity High availability Disaster recovery Technology Pillar Automation capability (both biz and IT processes) Automation platform Continuity of automation platform Control design in automation Identity and access management Development lifecycle Logging and monitoring License compliance Resilience of technologies Risk level (from legal, security, compliance standpoints) Availability of the 3rd party services Development/update lifecycle Details of benchmarking metrics The following weights of metrics and maturity values are defined based on my personal perspective from work experience, i.e. very subjective. Anyone who likes to adopt these metrics, should take the risk appetite of the organization into account and adjust them. Productivity Metrics Description Scope Weight in category Low maturity value Medium maturity value High maturity value Deployment frequency Number of deployments to production Past 6 months 5% &lt;=1 or &gt;24 &lt;=6 &lt;=24 Change volume Average number of user stories deployed in one sprint divided by working days of a sprint Past 6 months 10% &lt; 0.2 or &gt; 1 &gt;= 0.2 and &lt;= 0.3 &gt; 0.3 and &lt;= 1 Mean change lead time Mean time between a code commit and production deployment of that code Past 6 months 15% &gt; 1 month &gt;= 2 weeks &lt; 2 weeks Mean Time to realize value Mean time between a feature request and realization of business value from that feature Last 3 releases 10% &gt; 1.5 years &gt; 9 months &lt;= 9 months Mean time to operate Mean time between the beginning of Sprint 0 to achieving authority to operate Last 3 releases 20% &gt; 1 month &gt;= 2 weeks &lt; 2 weeks Test automation percentage Percentage of test cases automated. All active features 15% &lt; 50% &lt; 80% &gt;= 80% Developer onboarding mean time Time from a developer joining the team to ability to commit code for production deployment Past 1 year 15% &lt; 2 weeks or &gt;= 2 months &gt;= 1 month and &lt; 2 months &gt;= 2 weeks and &lt; 1 month Mean time from sourcing to contract Mean time from the beginning of sourcing process to contract sign-off Past 1 year 10% &gt; 3 months &gt; 1 month &lt;= 1 month Risk management capability Metrics Description Scope Weight in category Low maturity value Medium maturity value High maturity value Mean time to recovery Time between a failed production deployment to full restoration of production operations All active features 15% &gt; 1 day &gt; 1 hour &lt;= 1 hour Mean recovery point Mean time range of data that is lost due to an incident All active features 15% &gt; 1 day &gt; 1 hour &lt;= 1 hour Bug volume Average number of bugs reported after a release Last 3 releases 5% &gt;= 50 &gt; =10 and &lt; 50 &lt; 10 Bug Resolution time Mean time to resolve a bug Past 6 months 10% &gt; 3 days &gt; 1 day and &lt; 3 days &lt;= 1 day Mean time to patch vulnerability Mean time between identification of a vulnerability in the platform or application and successful production deployment of a patch Past 6 months 10% &gt; 3 weeks &gt; 1 week &lt;= 1 week Logging availability Percentage of systems that are logging. Platform &amp; application 10% &lt; 50% &lt; 80% 100% Monitoring availability Mean time from event generation to being available to support team Past 6 months 5% &gt; 1 hour &gt; 10 mins &lt;= 10 mins Deployment review percentage Percentage of deployments being reviewed from architecture or security Past 1 year 10% &lt; 50% &lt; 80% 100% Secure secret percentage Percentage of app secrets are securely created &amp; accessed without any human interactions. All active features 10% &lt; 50% &lt; 80% 100% Privilege auditing frequency Number of times that users and their privileges are audited Past 1 year 10% &lt;= 2 &gt; 2 and &lt;=12 &gt; 12 Usability Metrics Description Scope Weight in category Low maturity value Medium maturity value High maturity value Completion rate of user tasks Percentage of user tasks completed successfully versus total tasks All active features 35% &lt; 50% &lt; 80% 100% Mean time to complete user tasks Mean time for users to complete tasks All active features 35% &gt;10 mins &gt;1min and &lt;= 10 mins &lt;= 1 min Test level satisfaction Average level of overall impression of user experience (on a 5-point scale) Last 3 releases 30% &lt; 2 &lt; 4 &gt;= 4 Reliability Metrics Description Scope Weight in category Low maturity value Medium maturity value High maturity value Availability Amount of uptime in accordance with the SLA In a year 25% &lt;= 99% &lt; 99.99% &gt;= 99.99% Test coverage Percentage of code or feature that is covered by unit, integration, functional and security tests All active features 25% &lt; 50% &lt; 80% 100% Security benchmark deviation Deviation between security benchmarks applied and the baseline Platform &amp; application 25% &gt; 50% &lt;= 50% and &gt; 0% 0% #of MICS control deficiencies Number of key control deficiencies in MICS assessment report Latest report within the quarter 10% &gt;= 3 or no report &gt; 0 and &lt; 3 0 #of monitoring alerts Amount of monitoring alerts triggered Past 6 months 15% thousands hundreds tens]]></content>
      <tags>
        <tag>DevOps</tag>
        <tag>review framework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github建个人博客]]></title>
    <url>%2F2019%2F01%2F05%2Fhexo-github%2F</url>
    <content type="text"><![CDATA[注册一个Github账号，新建一个repository 命名方式&lt;Github用户名&gt;.github.io，比如我的库叫做yangxfang.github.io。操作完毕后将这个名字输入浏览器地址栏就可以访问个人博客网站啦。 至于repository的设置，这是个Public库 😃 ​ 在Settings里找到Github Pages，这里可以选择一个主题。这里就可以知道在Github里搭建个人博客依赖的技术就是Github Pages啦。 ​ 本地安装Hexo，发布到Github 安装依赖组件：Git、Node.js；Windows、Mac或Linux版本均有。 Mac上的命令如下 123sudo brew install gitsudo brew install nodejssudo brew install npm Node.js的安装也可以是： 1curl https://raw.github.com/creationix/nvm/master/install.sh | sh 安装Hexo 1npm install -g hexo-cli cd 你的个人博客源文件目录，也可以不用特别指定目录 1234hexo init myblog #自定义名字cd myblog #进入这个myblog目录npm install #配置nodehexo g &amp;&amp; hexo s #这会创建基础网页，启动Hexo服务，根据提示拷贝http://localhost:4000/进入浏览器查看，按ctrl+C停止服务 安装网站主题，Hexo网站上有非常多主题可选，我选了NexT（详细介绍见https://theme-next.org） 1git clone https://github.com/theme-next/hexo-theme-next themes/next #安装完成后可以在Themes目录下看到next目录 配置hexo，编辑_config.yml 1vi _config.yml #我用VIM编辑，当然你也可以用其他工具编辑 网站基本信息： 网站主题配置为next，部署方式为git，部署地址为github库的完整url，所属的库branch为master 当然你可以部署到其他branch，那就要去库的设置里把Pages的branch修改一下 配置主题，编辑themes/next/目录下的_config.yml NexT的扩展性很好，有很多插件可以启用，它的中英文网站有具体的说明，下方稍微列举下我配置的。 网页上可见的目录，默认只有home、archives，如需要就去掉#。 主题设置，默认Muse，我的是Gemini 启用回到顶部b2t，显示下滑条百分比scrollpercent 注意此选项仅对Pisces和Gemini主题有效 评论功能 😄，这里我使用了Valine 要用这个功能需要注册Leancloud（免费版即可），新建一个myblog的应用，在应用配置里找到appid和appkey，填在下方 同时还可以利用Leancloud来统计每篇文章的访客数量，可以使用相同的appid和appkey 还没完，在hexo的主_config.yml也需要配置 安装有关组件并注册： 12npm install hexo-leancloud-counter-security --savehexo lc-counter r 用户名 密码 启用分享到其他社交网站的功能 注意 很多功能是需要安装依赖包的，具体都会描述在Dependencies里，这就需要回到Hexo myblog目录来安装这些包，例如： 1git clone https://github.com/theme-next/theme-next-needmoreshare2 source/lib/needsharebutton 搜索功能 主页不显示全文，仅预览 配置文件那么长，自己多多发掘好用的功能吧 😃 终于要发布到Github了，让Github和本机相互信任一下 😃 生成SSH 123git config --global user.name "yourname" #用的是Github的用户名git config --global user.email "youremail" #用的是注册Github的邮箱地址ssh-keygen -t rsa -C "youremail" 在执行目录下找到公钥pub和私钥 回到Github的个人设置页面，创建一个新的SSH and GPG keys，将公钥内容完整复制到key里，完成后就长这样吧 部署Hexo 12npm install hexo-deployer-git --save #安装组件hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 部署完毕后，就可以在Github的库里看到所有的文件，打开网址yangxfang.github.io 就👌啦 写博客 新建博客 1hexo new post "标题" 新的md文件就会出现在source/_posts/目录下 编辑内容 md文件需要用Markdown编辑工具，我用的是Typora 在文章内插入图片，这真是折腾了我好一会。在_config.yml里找到配置post_asset_folder:改成true。修改之后，新建一个post时，hexo会自动建立一个同名的文件夹，在编辑md时把图片放在这个目录下即可。 在文章内插入目录， 首先安装组件 12npm i hexo-renderer-markdown-it --savenpm i hexo-renderer-markdown-it-plus --save 在_config.yml里添加如下内容 12345678910markdown_it_plus: highlight: true html: true xhtmlOut: true breaks: true langPrefix: linkify: true typographer: quotes: pre_class: highlight 在文章内需要插入目录的位置输入@[TOC] 发布 1hexo g &amp;&amp; hexo d 如果觉得github.io网址不够酷 那就自己买个域名，解析为Github的IP，在GitHub里设置自定义的域名，source目录下创建cname文件（无后缀），写进自定义的域名，最后，再发布一次Hexo到Github 1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d **博客最最最重要的当然就是内容，我要不断学习Markdown编辑方式，把自己的学习和心得体会都记录下来 ^ ^ **]]></content>
      <tags>
        <tag>how to</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Starting my blog]]></title>
    <url>%2F2019%2F01%2F01%2FStarting-my-blog%2F</url>
    <content type="text"><![CDATA[2019年计划 写个人博客 每天一杯咖啡 坚持义工写信 坚持每周跑步、健身 学会开车]]></content>
      <tags>
        <tag>Say Hi</tag>
      </tags>
  </entry>
</search>
